{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# BeautifulSoup\n",
    "- 간단하게 HTML 과 XML 에서 정보를 추출 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: beautifulsoup4 in c:\\users\\kdt\\anaconda3\\lib\\site-packages (4.12.2)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\users\\kdt\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n"
     ]
    }
   ],
   "source": [
    "# 설치 \n",
    "# !pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BeautifulSoup 기본 사용법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1>스크레이핑이란?</h1>\n",
      "<p>웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 사이트 주소 \n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam01.html\"\n",
    "\n",
    "# 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1: 스크레이핑이란?\n",
      "p1 : 웹 페이지를 분석하는 것\n",
      "p2 : 원하는 부분을 추출하는 것\n"
     ]
    }
   ],
   "source": [
    "# 원하는 부분 추출하기 \n",
    "h1 = soup.html.body.h1\n",
    "p1 = soup.html.body.p\n",
    "p2 = p1.next_sibling.next_sibling\n",
    "\n",
    "# 요소의 글자 추출하기 \n",
    "print('h1:', h1.string)   # string, text, get_text()\n",
    "print('p1 :', p1.string)\n",
    "print('p2 :', p2.string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<h1 id=\"title\">스크레이핑이란?</h1>\n",
      "<p id=\"body\">웹 페이지를 분석하는 것</p>\n",
      "<p>원하는 부분을 추출하는 것</p>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# id로 요소 찾는 방법 \n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 사이트 주소 \n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam02.html\"\n",
    "\n",
    "# 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#title= 스크레이핑이란?\n",
      "#body= 웹 페이지를 분석하는 것\n"
     ]
    }
   ],
   "source": [
    "title = soup.find(id='title')\n",
    "body = soup.find(id='body')\n",
    "\n",
    "print(\"#title=\", title.string)\n",
    "print(\"#body=\", body.string)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 여러개의 요소 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<ul>\n",
      "<li><a href=\"http://www.naver.com\">naver</a></li>\n",
      "<li><a href=\"http://www.daum.net\">daum</a></li>\n",
      "</ul>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "# id로 요소 찾는 방법 \n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 사이트 주소 \n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam03.html\"\n",
    "\n",
    "# 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<a href=\"http://www.naver.com\">naver</a>, <a href=\"http://www.daum.net\">daum</a>]\n",
      "naver > http://www.naver.com\n",
      "daum > http://www.daum.net\n"
     ]
    }
   ],
   "source": [
    "links = soup.find_all('a')\n",
    "print(links)\n",
    "\n",
    "for a in links:\n",
    "  # print(a)\n",
    "  text = a.string\n",
    "  href = a.attrs['href']\n",
    "  print(text, \">\", href)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 기상청 RSS 중 필요한 정보만 추출하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import warnings\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "기상청 육상 중기예보\n",
      "○ (하늘상태) 이번 예보기간에는 대체로 흐리겠습니다.<br />○ (기온) 이번 예보기간 아침 기온은 18~22도, 낮 기온 26~30도로 평년(최저기온 18~20도, 최고기온 26~29도)과 비슷하거나 높겠습니다.<br />          한편, 일 최고체감온도가 30도 내외로 올라 무더운 곳이 많겠으니, 야외활동 시 건강관리에 각별히 유의하기 바랍니다.<br />○ (해상) 서해중부해상의 물결은 0.5~1.5m로 일겠습니다.<br />○ (주말전망) 22일(토)~23일(일)은 대체로 흐리겠습니다. 아침 기온은 20~22도, 낮 기온은 27~30도가 되겠습니다.<br /><br />* 이번 예보기간에는 제주도 부근에 위치한 정체전선과 저기압 등 우리나라 주변 기압계 흐름에 따라 예보 변동성이 크겠으니, 앞으로 발표되는 최신 예보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "stnId ='109'\n",
    "url = f\"http://www.kma.go.kr/weather/forecast/mid-term-rss3.jsp?stnId={stnId}\"\n",
    "data = req.urlopen(url)\n",
    "\n",
    "soup = BeautifulSoup(data, 'html.parser')\n",
    "title = soup.find('title').string\n",
    "wf = soup.find('wf').string\n",
    "\n",
    "# 출력하기 \n",
    "print(title)\n",
    "print(wf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (하늘상태) 이번 예보기간에는 대체로 흐리겠습니다.\n",
      " (기온) 이번 예보기간 아침 기온은 18~22도, 낮 기온 26~30도로 평년(최저기온 18~20도, 최고기온 26~29도)과 비슷하거나 높겠습니다.\n",
      "          한편, 일 최고체감온도가 30도 내외로 올라 무더운 곳이 많겠으니, 야외활동 시 건강관리에 각별히 유의하기 바랍니다.\n",
      " (해상) 서해중부해상의 물결은 0.5~1.5m로 일겠습니다.\n",
      " (주말전망) 22일(토)~23일(일)은 대체로 흐리겠습니다.\n",
      "아침 기온은 20~22도, 낮 기온은 27~30도가 되겠습니다.\n",
      "\n",
      "* 이번 예보기간에는 제주도 부근에 위치한 정체전선과 저기압 등 우리나라 주변 기압계 흐름에 따라 예보 변동성이 크겠으니, 앞으로 발표되는 최신 예보를 참고하기 바랍니다.\n"
     ]
    }
   ],
   "source": [
    "for i in wf.split(\"<br />\"):\n",
    "  print(i.replace(\"○\",\"\").replace(\". \", \".\\n\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CSS 선택자 사용하기\n",
    "- Beautifulsoup 은 자바스크립트 라이브러리 인 JQuery 처럼 CSS 선택자를 지정해서 원하는 요소를 추출 가능 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"meigen\">\n",
      "<h1>위키북스 도서</h1>\n",
      "<ul class=\"items\">\n",
      "<li>유니티 게임 이펙트 입문</li>\n",
      "<li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
      "<li>모던 웹사이트 디자인의 정석</li>\n",
      "</ul>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# 사이트 주소 \n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam04.html\"\n",
    "\n",
    "# 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "print(soup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =  위키북스 도서\n",
      "[<li>유니티 게임 이펙트 입문</li>, <li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>, <li>모던 웹사이트 디자인의 정석</li>]\n",
      "유니티 게임 이펙트 입문\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n",
      "['유니티 게임 이펙트 입문', '스위프트로 시작하는 아이폰 앱 개발 교과서', '모던 웹사이트 디자인의 정석']\n",
      "['유니티 게임 이펙트 입문', '스위프트로 시작하는 아이폰 앱 개발 교과서', '모던 웹사이트 디자인의 정석']\n"
     ]
    }
   ],
   "source": [
    "# 필요한 부분을  CSS 쿼리로 추출하기 \n",
    "# (#: id, .:class, > : 자식, 빈칸 : 후손 )\n",
    "\n",
    "#  타이틀 하나만 추출하기 \n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(\"h1 = \",h1)\n",
    "\n",
    "\n",
    "li_data =[]\n",
    "for li in li_list:\n",
    "  print(li.string)\n",
    "  li_data.append(li.string)\n",
    "\n",
    "print(li_data)\n",
    "li_data2 = [li.string for li in li_list]\n",
    "print(li_data2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<html>\n",
      "<head>\n",
      "</head>\n",
      "<body>\n",
      "<div id=\"meigen\">\n",
      "<h1>위키북스 도서</h1>\n",
      "<ul class=\"items\">\n",
      "<li>유니티 게임 이펙트 입문</li>\n",
      "<li>스위프트로 시작하는 아이폰 앱 개발 교과서</li>\n",
      "<li>모던 웹사이트 디자인의 정석</li>\n",
      "</ul>\n",
      "</div>\n",
      "</body>\n",
      "</html>\n",
      "h1 =  위키북스 도서\n"
     ]
    }
   ],
   "source": [
    "# 라이브러리 불러오기 \n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "\n",
    "# 사이트 주소 \n",
    "url = \"https://zeushahn.github.io/Test/python/bs_exam04.html\"\n",
    "\n",
    "# html Data 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "print(soup)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h1 =  위키북스 도서\n"
     ]
    }
   ],
   "source": [
    "\n",
    "h1 = soup.select_one(\"div#meigen > h1\").string\n",
    "print(\"h1 = \",h1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "유니티 게임 이펙트 입문\n",
      "스위프트로 시작하는 아이폰 앱 개발 교과서\n",
      "모던 웹사이트 디자인의 정석\n"
     ]
    }
   ],
   "source": [
    "li_list = soup.select('ul.items > li')\n",
    "\n",
    "for li in li_list:\n",
    "  print(li.string)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 네이버 금융에서 환율 정보 추출하기 \n",
    "- https://finance.naver.com/marketindex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw :  1,381.00\n"
     ]
    }
   ],
   "source": [
    "# 미국 환율 가져오기 \n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "# html Data 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "# 데이터 추출하기 \n",
    "price = soup.select_one('#exchangeList > li.on > a.head.usd > div > span.value')\n",
    "print(\"usd/krw : \", price.string)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usd/krw :  1,380.50\n",
      "jpa/krw :  874.48\n",
      "eur/krw :  1,484.04\n",
      "cny/krw :  189.71\n",
      "------------------------------------------\n",
      "usd/krw : 1,380.50\n",
      "jpy/krw : 874.48\n",
      "eur/krw : 1,484.04\n",
      "cny/krw : 189.71\n",
      "------------------------------------------\n",
      "미국 : 1,380.50\n",
      "일본 : 874.48\n",
      "유럽연합 : 1,484.04\n",
      "중국 : 189.71\n"
     ]
    }
   ],
   "source": [
    "# 미국, 일본, 유럽연합, 중국 환율 가져오기 \n",
    "\n",
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://finance.naver.com/marketindex/\"\n",
    "\n",
    "# html Data 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "# 데이터 추출하기 ( li.on 에서 on 을 빼니까 값이 나옴 )\n",
    "us_price = soup.select_one(\"#exchangeList > li > a.head.usd > div > span.value\")\n",
    "jp_price = soup.select_one(\"#exchangeList > li > a.head.jpy > div > span.value\")   \n",
    "eu_price = soup.select_one(\"#exchangeList > li > a.head.eur > div > span.value\")\n",
    "cn_price = soup.select_one(\"#exchangeList > li > a.head.cny > div > span.value\")\n",
    "\n",
    "print(\"usd/krw : \", us_price.string)\n",
    "print(\"jpa/krw : \", jp_price.string)\n",
    "print(\"eur/krw : \", eu_price.string)\n",
    "print(\"cny/krw : \", cn_price.string)\n",
    "\n",
    "print (\"------------------------------------------\")\n",
    "# 다른방식으로 추출해 보기 \n",
    "exchangeList = [ 'usd', 'jpy', 'eur', 'cny']\n",
    "\n",
    "for li in exchangeList:\n",
    "    price = soup.select_one(f\"#exchangeList > li > a.head.{li} > div > span.value\")\n",
    "    print(f\"{li}/krw :\", price.string)\n",
    "\n",
    "print (\"------------------------------------------\")\n",
    "prices = soup.select('span.value')\n",
    "contry = ['미국','일본','유럽연합','중국']\n",
    "\n",
    "i = 0 \n",
    "for price in prices:\n",
    "    print(f\"{contry[i]} : {price.string}\")\n",
    "    i+= 1\n",
    "    if  i > len(contry)-1:\n",
    "        break\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 다음 IT News 많이 본 뉴스\n",
    "- https://media.daum.net/digital"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://v.daum.net/v/20240619180220422 : 갤럭시 S-갤럭시 폴드, 왜 지문인식 방법이 다를까\n",
      "https://v.daum.net/v/20240619175006925 : 美서는 안쓰는 기술인데…韓정부 전략에 우려 쏟아진 이유\n",
      "https://v.daum.net/v/20240619170013140 : 커피 좋아하는 습관도 '유전'?\n",
      "https://v.daum.net/v/20240619170002112 : UN '달 탐사 관련 국제 콘퍼런스'에 韓우주항공청 참여\n",
      "https://v.daum.net/v/20240619173611466 : 생성AI-클라우드 기술독립 키워드 'AI-PaaS'...\"韓 시장은 위축 중\"\n",
      "https://v.daum.net/v/20240619173346391 : 'AI폰 전초전' OS 업데이트 경쟁 뜨겁다\n",
      "https://v.daum.net/v/20240619171923914 : 갤럭시북4 엣지, AI·배터리·발열 최소화는 '합격'…LoL은 언제 되나\n",
      "https://v.daum.net/v/20240619171348741 : \"AI 패권 전쟁, 韓기업 생존하려면 플랫폼 중심으로 관점 바꿔야\"(종합)\n",
      "https://v.daum.net/v/20240619155709776 : [동영상] MS는 못해도 팀왈도는 한다! 스타필드 한글화로 관심UP\n",
      "https://v.daum.net/v/20240619170153252 : AI로 돌아가는 스마트시티…\"도시 디지털 인프라 보안도 필수\"\n"
     ]
    }
   ],
   "source": [
    "import urllib.request as req\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://media.daum.net/digital/\"\n",
    "\n",
    "# html Data 가져오기 \n",
    "res = req.urlopen(url)\n",
    "\n",
    "# BeautifulSoup 으로 분석하기 \n",
    "soup = BeautifulSoup(res, 'html.parser')\n",
    "\n",
    "a_list = soup.select(\"body > div.container-doc.cont-category > main > section > div.main-sub > div.box_g.box_news_major > ul > li a\") # li의 후순 a \n",
    "# print(a_list)\n",
    "\n",
    "for a in a_list:\n",
    "  # print(a)\n",
    "  href = a.attrs['href']\n",
    "  title = a.string\n",
    "  print(href, \":\", title)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
